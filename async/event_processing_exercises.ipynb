{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "795f209e",
   "metadata": {},
   "source": [
    "# Event‑Processing Exercises (Python Stdlib Only)\n",
    "\n",
    "Each section introduces a realistic “events‑in‑memory” problem and solves it using **exactly** one or more items from the cheat‑sheet you’ll want for a 60‑minute live‑coding interview.\n",
    "\n",
    "Feel free to run each cell as‑is, tweak the synthetic data sizes, or comment things out to profile performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd07ce0",
   "metadata": {},
   "source": [
    "## Helper: Synthetic Event Generator\n",
    "\n",
    "Run this once—subsequent problems re‑use the `generate_events()` helper so the data shape stays consistent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a56ed78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 0, 'ts': '2025-06-12T08:08:53.889057', 'user': 'user8', 'type': 'CLICK', 'payload': {'value': 760, 'notes': 'olMJUevblAbkHClEQaPK'}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random, string, time, json, csv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "EVENT_TYPES = [\"CLICK\", \"VIEW\", \"LOGIN\", \"PURCHASE\", \"LOGOUT\"]\n",
    "\n",
    "def random_timestamp(start: datetime, end: datetime):\n",
    "    \"\"\"Return a random datetime between *start* and *end*.\"\"\"\n",
    "    delta = end - start\n",
    "    random_seconds = random.randint(0, int(delta.total_seconds()))\n",
    "    return start + timedelta(seconds=random_seconds)\n",
    "\n",
    "def generate_events(n: int):\n",
    "    \"\"\"Generate *n* synthetic event dicts with fields id, ts, user, type.\"\"\"\n",
    "    start = datetime.now() - timedelta(days=1)\n",
    "    end = datetime.now()\n",
    "    for i in range(n):\n",
    "        yield {\n",
    "            \"id\": i,\n",
    "            \"ts\": random_timestamp(start, end).isoformat(),\n",
    "            \"user\": f\"user{random.randint(1, 50)}\",\n",
    "            \"type\": random.choice(EVENT_TYPES),\n",
    "            \"payload\": {\n",
    "                \"value\": random.randint(1, 1000),\n",
    "                \"notes\": ''.join(random.choices(string.ascii_letters, k=20))\n",
    "            }\n",
    "        }\n",
    "\n",
    "# miniature sanity check\n",
    "sample = next(generate_events(1))\n",
    "print(sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b43e499",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Problem 1 – Top‑K Event Types   (`collections.Counter`, `heapq.nlargest`)\n",
    "\n",
    "> **Task**   Given *N* events, return the *k* most common `type` values.\n",
    "\n",
    "These two stdlib utilities let you compute frequencies in O(N) and pick the largest K in O(M log K).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7009215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "import heapq\n",
    "\n",
    "events = list(generate_events(10_000))\n",
    "k = 3\n",
    "\n",
    "type_counts = Counter(e['type'] for e in events)\n",
    "top_k = heapq.nlargest(k, type_counts.items(), key=lambda kv: kv[1])\n",
    "\n",
    "print(f\"Top-{k} event types:\", top_k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5099e6b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Problem 2 – Per‑User Sliding‑Window Rate‑Limiter   (`collections.defaultdict`, `collections.deque`)\n",
    "\n",
    "> **Task**   Allow at most **X** events per user in any rolling 60‑second window.\n",
    "\n",
    "The `defaultdict` gives us a bucket per user; each bucket is a `deque` so we can pop expired timestamps in O(1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e681d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict, deque\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "WINDOW = timedelta(seconds=60)\n",
    "LIMIT = 20\n",
    "\n",
    "buckets: dict[str, deque] = defaultdict(deque)\n",
    "violations = 0\n",
    "\n",
    "for ev in generate_events(5_000):\n",
    "    user = ev['user']\n",
    "    ts = datetime.fromisoformat(ev['ts'])\n",
    "    dq = buckets[user]\n",
    "\n",
    "    # expire old\n",
    "    while dq and ts - dq[0] > WINDOW:\n",
    "        dq.popleft()\n",
    "\n",
    "    dq.append(ts)\n",
    "    if len(dq) > LIMIT:\n",
    "        violations += 1\n",
    "\n",
    "print(f\"Violations detected: {violations}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba48425",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Problem 3 – LRU Cache for Event Payloads   (`collections.OrderedDict`)\n",
    "\n",
    "> **Task**   Memoise expensive transformations of `payload` using a 128‑entry LRU.\n",
    "\n",
    "`OrderedDict` lets us pop the *oldest* item in O(1) while keeping look‑up O(1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4830f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import OrderedDict\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "CAPACITY = 128\n",
    "cache: OrderedDict[str, dict] = OrderedDict()\n",
    "\n",
    "def expensive_transform(payload: dict) -> dict:\n",
    "    # pretend this is slow\n",
    "    key = hashlib.sha256(json.dumps(payload, sort_keys=True).encode()).hexdigest()\n",
    "    if key in cache:\n",
    "        # mark as most recently used\n",
    "        cache.move_to_end(key)\n",
    "        return cache[key]\n",
    "\n",
    "    # Fake expensive computation\n",
    "    result = {**payload, \"score\": sum(payload.values()) if 'value' in payload else 0}\n",
    "\n",
    "    cache[key] = result\n",
    "    # evict\n",
    "    if len(cache) > CAPACITY:\n",
    "        cache.popitem(last=False)\n",
    "    return result\n",
    "\n",
    "# warm the cache\n",
    "for ev in generate_events(300):\n",
    "    expensive_transform(ev['payload'])\n",
    "\n",
    "print(f\"Cache size after warm‑up: {len(cache)} (should be ≤ {CAPACITY})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3088fa14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Problem 4 – Merge K Pre‑sorted Event Streams   (`heapq.merge`, `operator.itemgetter`)\n",
    "\n",
    "> **Task**   Given *K* sorted lists of events (by timestamp), produce a single sorted iterator.\n",
    "\n",
    "`heapq.merge` does a k‑way streaming merge in O(N log K) time without materialising intermediate lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa15ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import heapq, operator\n",
    "\n",
    "# Build 3 sorted streams\n",
    "streams = [sorted(generate_events(1_000), key=operator.itemgetter('ts')) for _ in range(3)]\n",
    "\n",
    "merged = heapq.merge(*streams, key=operator.itemgetter('ts'))\n",
    "\n",
    "# Pull the first 5 to show order\n",
    "for _ in range(5):\n",
    "    print(next(merged)['ts'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfa5f52",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Problem 5 – Count Events in O(log N) Sliding Window   (`bisect`)\n",
    "\n",
    "> **Task**   Insert timestamps into a **sorted** list and answer “how many events occurred in the last *W* seconds?” quickly.\n",
    "\n",
    "`bisect_left / bisect_right` give us binary‑search insertion & range counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c02d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bisect import bisect_left, bisect_right, insort\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "WINDOW = timedelta(minutes=5)\n",
    "timestamps: list[datetime] = []\n",
    "\n",
    "def insert(ev_ts: datetime):\n",
    "    insort(timestamps, ev_ts)\n",
    "\n",
    "def count_last_window(now: datetime) -> int:\n",
    "    start = now - WINDOW\n",
    "    left = bisect_left(timestamps, start)\n",
    "    right = bisect_right(timestamps, now)\n",
    "    return right - left\n",
    "\n",
    "# Simulate\n",
    "now = datetime.now()\n",
    "for ev in generate_events(2_000):\n",
    "    ts = datetime.fromisoformat(ev['ts'])\n",
    "    insert(ts)\n",
    "\n",
    "print(\"Events in last 5 minutes:\", count_last_window(now))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af51c24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Problem 6 – Daily Cumulative Counts   (`itertools.groupby`, `itertools.accumulate`, `datetime`)\n",
    "\n",
    "> **Task**   Group events by **calendar day** and emit a running total per day.\n",
    "\n",
    "`groupby` gives batches; `accumulate` keeps a running sum for dashboards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cda423bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_events' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitertools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m groupby, accumulate\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m events = [(datetime.fromisoformat(ev[\u001b[33m'\u001b[39m\u001b[33mts\u001b[39m\u001b[33m'\u001b[39m]).date(), ev) \u001b[38;5;28;01mfor\u001b[39;00m ev \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgenerate_events\u001b[49m(\u001b[32m3_000\u001b[39m)]\n\u001b[32m      6\u001b[39m events.sort(key = \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m0\u001b[39m])\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m day, group \u001b[38;5;129;01min\u001b[39;00m groupby(events, key= \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m0\u001b[39m]):\n",
      "\u001b[31mNameError\u001b[39m: name 'generate_events' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from itertools import groupby, accumulate\n",
    "from datetime import datetime\n",
    "\n",
    "events = [(datetime.fromisoformat(ev['ts']).date(), ev) for ev in generate_events(3_000)]\n",
    "\n",
    "events.sort(key = lambda x: x[0])\n",
    "\n",
    "for day, group in groupby(events, key= lambda x: x[0]):\n",
    "    print(f\"{day}: {len(list(group))}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b50d8b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Problem 7 – Memoised Payload Normalisation   (`functools.lru_cache`)\n",
    "\n",
    "> **Task**   Normalise a JSON payload (expensive) but many duplicates exist.\n",
    "\n",
    "`lru_cache` hides the boilerplate from Problem 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828b2533",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from functools import lru_cache\n",
    "import json, hashlib\n",
    "\n",
    "@lru_cache(maxsize=256)\n",
    "def normalise(raw_json: str) -> dict:\n",
    "    data = json.loads(raw_json)\n",
    "    # pretend this is heavy\n",
    "    data['checksum'] = hashlib.md5(raw_json.encode()).hexdigest()\n",
    "    return data\n",
    "\n",
    "dupes = [json.dumps({'x': 1, 'y': 2})] * 10\n",
    "for doc in dupes:\n",
    "    normalise(doc)\n",
    "\n",
    "print(\"Cache info:\", normalise.cache_info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b28ce96",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Problem 8 – Parse NDJSON → CSV   (`json`, `csv`)\n",
    "\n",
    "> **Task**   Read newline‑delimited JSON events and write selected fields to CSV.  \n",
    "> Only stdlib allowed—so no `pandas`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b75885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import io, json, csv, random\n",
    "\n",
    "# Create fake NDJSON string\n",
    "ndjson_blob = '\\n'.join(json.dumps(ev) for ev in generate_events(100))\n",
    "\n",
    "# Parse & write to CSV in‑memory\n",
    "csv_buf = io.StringIO()\n",
    "writer = csv.writer(csv_buf)\n",
    "writer.writerow(['id', 'ts', 'user', 'type'])  # header\n",
    "\n",
    "for line in ndjson_blob.splitlines():\n",
    "    ev = json.loads(line)\n",
    "    writer.writerow([ev['id'], ev['ts'], ev['user'], ev['type']])\n",
    "\n",
    "csv_content = csv_buf.getvalue().splitlines()[:5]\n",
    "print(\"Preview CSV rows:\")\n",
    "for row in csv_content:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69de4162",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Problem 9 – Multi‑key Sort (timestamp, type, user)   (`operator.itemgetter`)\n",
    "\n",
    "> **Task**   Return the first 10 events ordered by `(ts, type, user)` in one pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ffbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import operator\n",
    "\n",
    "events = list(generate_events(200))\n",
    "events_sorted = sorted(events, key=operator.itemgetter('ts', 'type', 'user'))\n",
    "\n",
    "for ev in events_sorted[:10]:\n",
    "    print(ev['ts'], ev['type'], ev['user'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82347dbb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Problem 10 – Finding the Median Event `value` on the Fly   (`heapq` again, but *dual* heaps)\n",
    "\n",
    "> **Task**   Stream events and be able to query the median of the integer `payload['value']` at any time.\n",
    "\n",
    "A classic interview favourite that uses **two heaps**—max‑heap for the lower half, min‑heap for the upper half.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d0886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import heapq\n",
    "\n",
    "low, high = [], []  # max‑heap (invert values) & min‑heap\n",
    "\n",
    "def add(value: int):\n",
    "    if not low or value <= -low[0]:\n",
    "        heapq.heappush(low, -value)\n",
    "    else:\n",
    "        heapq.heappush(high, value)\n",
    "\n",
    "    # rebalance\n",
    "    if len(low) > len(high) + 1:\n",
    "        heapq.heappush(high, -heapq.heappop(low))\n",
    "    elif len(high) > len(low):\n",
    "        heapq.heappush(low, -heapq.heappop(high))\n",
    "\n",
    "def median() -> float:\n",
    "    if len(low) == len(high):\n",
    "        return (-low[0] + high[0]) / 2\n",
    "    return float(-low[0])\n",
    "\n",
    "for ev in generate_events(1_000):\n",
    "    add(ev['payload']['value'])\n",
    "\n",
    "print(\"Median payload value:\", median())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
